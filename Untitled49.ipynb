{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOIhhT07eIguAsLzm8SBq7h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"id":"8RJbizPFSfw_","executionInfo":{"status":"ok","timestamp":1738327591033,"user_tz":-330,"elapsed":12112,"user":{"displayName":"Aryan","userId":"02076124039323163143"}}},"outputs":[],"source":["import os\n","# Torch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader,WeightedRandomSampler\n","from sklearn.utils.class_weight import compute_class_weight\n","# Torch Vision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","# Transformers\n","from transformers import ViTImageProcessor, ViTForImageClassification\n","# Utils\n","from tqdm import tqdm\n","import numpy as np\n","# Train Test Evaluate\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","# Visualization\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["g_vit_patch = 'google/vit-base-patch16-224'\n","base_path = '/kaggle/input/ecg-image-data' # For training Data\n","base_output_path = '/kaggle/working' # For Storing weights and all\n","# Set device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_epochs = 1\n","NUM_WORKERS = 2"],"metadata":{"id":"ZOzVBoRkSwlC","executionInfo":{"status":"ok","timestamp":1738327752248,"user_tz":-330,"elapsed":732,"user":{"displayName":"Aryan","userId":"02076124039323163143"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class CNNModel(nn.Module):\n","    def __init__(self, num_classes, device, cnn_model=models.resnet18(weights='ResNet18_Weights.DEFAULT')):\n","        super(CNNModel, self).__init__()\n","        self.cnn = cnn_model\n","        num_ftrs = self.cnn.fc.in_features\n","        self.cnn.fc = nn.Linear(num_ftrs, num_classes)\n","        self.device = device\n","\n","    def forward(self, x):\n","        # Ensure input is on the correct device\n","        x = x.to(self.device)\n","        return self.cnn(x)\n","\n","# ViT Model with Explicit Device Handling\n","class ViTModel(nn.Module):\n","    def __init__(self, num_classes, device, vit_model_name=g_vit_patch):\n","        super(ViTModel, self).__init__()\n","        self.feature_extractor = ViTImageProcessor.from_pretrained(vit_model_name,do_rescale=False)\n","        self.model = ViTForImageClassification.from_pretrained(\n","            vit_model_name,\n","            num_labels=num_classes,\n","            ignore_mismatched_sizes=True,\n","        )\n","        self.device = device\n","\n","    def forward(self, x):\n","        # Ensure input is on the correct device\n","        x = x.to(self.device)  # Move to the specified device\n","        # Prepare inputs for ViT model\n","        feature_input = self.feature_extractor(x, return_tensors=\"pt\").to(self.device)\n","        logits = self.model(**feature_input).logits\n","        return logits\n","\n","# Ensemble Model with Explicit Device Handling\n","class EnsembleModel(nn.Module):\n","    def __init__(self, num_classes, cnn_model, vit_model, device):\n","        super(EnsembleModel, self).__init__()\n","        self.cnn_model = cnn_model\n","        self.vit_model = vit_model\n","        self.fc = nn.Linear(num_classes * 2, num_classes)  # Final linear layer\n","        self.device = device\n","\n","    def forward(self, x):\n","        # Ensure input is on the correct device\n","        x = x.to(self.device)\n","\n","        # Forward through CNN and ViT\n","        cnn_output = self.cnn_model(x)  # CNN output\n","        vit_output = self.vit_model(x)  # ViT output\n","\n","        # Combine outputs\n","        combined_output = torch.cat((cnn_output, vit_output), dim=1)  # Concatenate\n","\n","        # Final linear output\n","        return self.fc(combined_output)\n","\n","    def eval(self, *args, **kwargs):\n","        super().eval(*args, **kwargs)  # Call eval() on the parent class\n","\n","        # Call eval() on the underlying models\n","        self.cnn_model.eval(*args, **kwargs)\n","        self.vit_model.eval(*args, **kwargs)\n","\n","    def train(self, *args, **kwargs):\n","        super().train( *args, **kwargs)  # Call train() on the parent class\n","\n","        # Call train() on the underlying models\n","        self.cnn_model.train( *args, **kwargs)\n","        self.vit_model.train( *args, **kwargs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edDBO9ZPSykF","executionInfo":{"status":"ok","timestamp":1738327769549,"user_tz":-330,"elapsed":1340,"user":{"displayName":"Aryan","userId":"02076124039323163143"}},"outputId":"e64d4eeb-9510-42b8-8940-f4ba7608dc65"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 170MB/s]\n"]}]},{"cell_type":"code","source":["def preprocess(base_path: str, device: torch.device):\n","    # Your transformations\n","    train_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ToTensor(),\n","    ])\n","\n","    test_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    print(\"Loading Datasets...\")\n","    # Load datasets\n","    train_dataset = datasets.ImageFolder(\n","        root=os.path.join(base_path, \"train\"),\n","        transform=train_transform\n","    )\n","\n","    test_dataset = datasets.ImageFolder(\n","        root=os.path.join(base_path, \"test\"),\n","        transform=test_transform\n","    )\n","\n","    print(\"Balancing Training Datasets...\")\n","\n","    # Assuming you have a dataset 'dataset' with labels 'labels'\n","    # Calculate class counts\n","    labels = torch.tensor(train_dataset.targets)\n","    train_class_counts = torch.bincount(labels)\n","    train_desired_count = torch.min(train_class_counts)\n","\n","    # Calculate weights for each sample based on class counts and desired count\n","    weights = train_desired_count / train_class_counts.float()  # Adjusted weights based on desired count\n","\n","    sample_weights = weights[labels]\n","\n","    # Create WeightedRandomSampler with the calculated weights\n","    num_samples = int(train_desired_count * len(train_class_counts))\n","    train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=num_samples, replacement=True)\n","\n","    print(\"Preparing the loaders...\")\n","    # Load data using DataLoader with the specified sampler\n","    train_loader = DataLoader(train_dataset, num_workers=4, batch_size=75, sampler=train_sampler)\n","    test_loader = DataLoader(test_dataset, num_workers=4, batch_size=75, shuffle=True)\n","\n","    print(f\"{train_class_counts=} {train_desired_count=}\")\n","    print(\"All Done...\")\n","\n","    return train_dataset, test_dataset, train_loader, test_loader\n"],"metadata":{"id":"2CY0ctWQaLnJ","executionInfo":{"status":"ok","timestamp":1738327782177,"user_tz":-330,"elapsed":483,"user":{"displayName":"Aryan","userId":"02076124039323163143"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["data_folder = os.path.join(base_path, 'ECG_Image_data')\n","\n","train_dataset, test_dataset, train_loader, test_loader = preprocess(data_folder, device)\n"],"metadata":{"id":"aJk6Z3hCaO5I","executionInfo":{"status":"error","timestamp":1738327789742,"user_tz":-330,"elapsed":646,"user":{"displayName":"Aryan","userId":"02076124039323163143"}},"outputId":"0ec779a3-ef75-4335-fb5b-e8b1ae721bb9","colab":{"base_uri":"https://localhost:8080/","height":339}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Datasets...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/ecg-image-data/ECG_Image_data/train'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-7a3d596437ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ECG_Image_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-9c57e235fdf9>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(base_path, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Datasets...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     train_dataset = datasets.ImageFolder(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ecg-image-data/ECG_Image_data/train'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"g5iT0gv-aQtZ"},"execution_count":null,"outputs":[]}]}